{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# HPOBench\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>This page is under construction.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import glob\nfrom io import StringIO\n\nimport pandas as pd\n\nfrom hpobench.benchmarks.ml.tabular_benchmark import TabularBenchmark\n\n# COPY PASTE the table from HPOBENCH table 7\ndataset_table = \"\"\"\nname tid #obs #feat\nblood-transf 10101 748 4\nvehicle 53 846 18\nAustralian 146818 690 14\ncar 146821 1728 6\nphoneme 9952 5404 5\nsegment 146822 2310 19\ncredit-g 31 1000 20\nkc1 3917 2109 22\nsylvine 168912 5124 20\nkr-vs-kp 3 3196 36\njungle_che 167119 44819 6\nmfeat-factors 12 2000 216\nshuttle 146212 58000 9\njasmine 168911 2984 145\ncnae-9 9981 1080 856\nnumerai28.6 167120 96320 21\nbank-mark 14965 45211 16\nhiggs 146606 98050 28\nadult 7592 48842 14\nnomao 9977 34465 118\n\"\"\"\n\ndf = pd.read_csv(StringIO(dataset_table), sep=\" \")\n\n# installing once by selecting the model by name  ['lr', 'svm', 'xgb', 'rf', 'nn']\n# then figure out the task_id: look at $HOME/.local/share/hpobench/TabularData/<model>.\n# the displayed folders are the task ids!\n\n# download the \"tasks\"\nmodels = [\"nn\", \"rf\", \"svm\", \"xgb\", \"lr\"]\nfor model in models:\n    TabularBenchmark(model=model, task_id=146821)\n\n# find available dataset ids\npath = \"/home/ruhkopf/.local/share/hpobench/TabularData/{model}/[0-9]*\"\ndataset_ids = {}\nfor model in models:\n    s = set(int(path.split(\"/\")[-1]) for path in glob.glob(path.format(model=model)))\n    dataset_ids[model] = s\n\ndataset_ids_nn = dataset_ids[\"nn\"]\n\n# find the joint set of datasets used for all but the nn algo\ndataset_ids = list(set.intersection(*[dataset_ids[model] for model in [\"rf\", \"svm\", \"xgb\", \"lr\"]]))\n\nmodel = \"lr\"\ndataset = dataset_ids[2]\nseed = [8916, 1319, 7222, 7541, 665][0]\nsubsample = 1.0\n\ntab = TabularBenchmark(model=model, task_id=dataset)\nprint(\"available fidelities: \\n\", tab.get_fidelity_space())\n\nconfig = tab.get_configuration_space(seed=seed).sample_configuration()  # fixme: kill sample_config\nresult = tab.objective_function(configuration=config, fidelity={\"subsample\": subsample}, rng=seed)\n\n# available seeds: (which are identical across datasets & algos\nresult[\"info\"].keys()  # [8916, 1319, 7222, 7541, 665]\n\nfor model in [\"rf\", \"svm\", \"xgb\", \"lr\"]:\n    for dataset in dataset_ids:\n        tab = TabularBenchmark(model=model, task_id=dataset)\n        fid = tab.get_fidelity_space()\n        print(f\"Model {model}, {dataset}, subsample:{fid._hyperparameters['subsample'].sequence}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}