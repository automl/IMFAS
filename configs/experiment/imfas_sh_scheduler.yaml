# @package _global_

# Transformer model.
# d = MLP_d(D), t = TransformerEncoder(lcs_DA), output = MLP(concat(d,t))

defaults:
  - /model: imfas_transformer


wandb:
  group: 'imfas_transformer'
  job_type: 'train'
  notes: 'imfas transformer'
  tags:
    - 'IMFAS Transformer'


# dataset:
#   train_dataset_class:
#     _target_: imfas.data.Dataset_Join_Dmajor
#     enable masking
#     masking_fn:
#       _partial_: True
#       _target_: imfas.utils.masking.mask_lcs_randomly


#   train_dataloader_class:
#     batch_size: 10

#   valid_dataloader_class:
#     batch_size: 100

#   test_dataloader_class:
#     batch_size: 100

model:
  encoder:
    hidden_dims:
      - ${ dynamically_computed.n_data_meta_features }
      - 300
      - 200

  n_algos: ${dynamically_computed.n_algos}
  n_layers: 2

  transformer_layer:
    _target_: torch.nn.TransformerEncoderLayer
    d_model: 128
    nhead: 4
    dim_feedforward: 256
    dropout: 0.2
    activation: 'relu'
    batch_first: True
    norm_first: True

  decoder:
    hidden_dims:
      # - ${add:dynamically_computed.n_algos,dynamically_computed.n_algos}
      - 328               # FIXME: dynamically compute based on the
      # concat dim before the tensor comes to the decoder
      # - 328
      - ${dynamically_computed.n_algos}


trainer:
  _target_: imfas.trainer.sh_budget_trainer.SHBudgetTrainer
  sh_model:
    eta: 2
    budgets: ${dataset.slices}

  run_call:
    epochs: 5000
    log_freq: 5         # wandb mistakes single value additions as media files, so log_freq helps mitigate that
    train_loss_fn:
      _target_: imfas.losses.spearman.SpearmanLoss

    test_loss_fns:

      top1_regret:
        _target_: imfas.evaluation.topkregret.TopkRegret
        k: 1

      top3_regret:
        _target_: imfas.evaluation.topkregret.TopkRegret
        k: 3

      top5_regret:
        _target_: imfas.evaluation.topkregret.TopkRegret
        k: 5
