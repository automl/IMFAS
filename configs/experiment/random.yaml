# @package _global_

# The general idea here is, that SuccessiveHalving is a non-parametric, myopic, but
# learning curve aware algorithm.
# Since we only ever care for its baseline (test) performance, this pipe is a bit hacky.


wandb:
  group: 'RandomBaseline'
  job_type: 'random'
  notes: 'Random Baseline'

device: 'cpu'

model:
  _target_: imfas.models.baselines.randombaseline.RandomBaseline


dataset:
  test_dataloader_class:
    batch_size: 1 # has to be one for successive halving. aggregate over test set in trainer.evaluate
    shuffle: False # to know on which dataset in test we perform badly

trainer:
  trainerobj:
    _target_: imfas.trainer.random_trainer.RandomTrainer
    reps: 1000
  run_call:
    epochs: 0 # since we only need a single test execution anyways (HACK)

#    test_loss_fns:
#      spearman:
#        #        _target_: imfas.losses.plackett_luce.PlackettLuceLoss
#        _target_: imfas.losses.spearman.SpearmanLoss