# @package _global_

# MLP(D) ---> PlackettLuceLoss
defaults:
  - _self_
  - override /dataset: lcbench
  - override /dataset/dataset_meta: lcbench_minimal
  - override /dataset/algo_meta: lcbench_minimal
  - override /trainer: basetrainer
  - override /model: imfas_wp


dataset:
  train_dataset_class:
    _target_: imfas.data.Dataset_Join_Dmajor
    masking_fn:
      _target_: imfas.utils.masking.mask_lcs_randomly
      _partial_: True

  test_dataset_class:
    _target_: imfas.data.Dataset_Join_Dmajor
    # TODO @TIM alex proposed to hold it fix for test, but do the imfas procedure

  train_dataloader_class:
    batch_size: 10

  test_dataloader_class:
    batch_size: 10

model:
  _target_: imfas.models.plackett_test.PlackettTest
  encoder:
    _target_: imfas.utils.mlp.MLP
    hidden_dims:
      - ${dynamically_computed.n_data_meta_features}
      - ${dynamically_computed.n_algos}

trainer:
  trainerobj:
    _target_: imfas.trainer.base_trainer.BaseTrainer # todo switch out for Slice evaluator!

    optimizer:
      _target_: torch.optim.Adam
      lr: 0.001
      _partial_: True

  run_call:
    epochs: 1000
    train_loss_fn:
      _target_: imfas.losses.plackett_luce.PlackettLuceLoss




wandb:
  group: imfas_wp
  notes: 'plackett test'
  mode: 'offline'