# @package _global_

# Transformer model.
# d = MLP_d(D), t = TransformerEncoder(lcs_DA), output = MLP(concat(d,t))

defaults:
  - override /dataset: lcbench
  - override /dataset/dataset_meta: lcbench_minimal
  - override /dataset/algo_meta: lcbench_minimal
  - override /model: imfas_transformer
  - override /trainer: basetrainer

dataset:
  train_dataset_class:
    _target_: imfas.data.Dataset_Join_Dmajor
    masking_fn:
      _partial_: True
      _target_: imfas.data.mask_lcs_randomly

  test_dataset_class:
    _target_: imfas.data.Dataset_Join_Dmajor


  dataloader_class:
    batch_size: 10

model:
  n_algos:
    ${dynamically_computed.n_algos}

  encoder:
    mlp_dims: # [ 107, 100 ]
      - ${dynamically_computed.n_data_meta_features}
      - 100

  decoder:
    mlp_dims: [ 100, 100 ]

  transformer_layer:
    _target_: torch.nn.TransformerEncoderLayer
    d_model: 128
    nhead: 4
    dim_feedforward: 256
    dropout: 0.2
    activation: 'relu'
    batch_first: True
    norm_first: True


wandb:
  group: imfas_wp
  notes: 'imfas wp'
  mode: 'offline'