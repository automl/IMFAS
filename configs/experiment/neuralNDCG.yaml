# @package _global_

# MLP(D) ---> NeuralNDCG@k
# FIXME: requires a 'python -m pip install git+https://github.com/allegro/allRank' install

defaults:
  - override /dataset: lcbench
  - override /dataset/dataset_meta: lcbench_minimal
  - override /dataset/algo_meta: lcbench_minimal
  - override /trainer: basetrainer
  - override /model: imfas_wp


wandb:
  notes: 'neuralNDCG test'


dataset:
  train_dataset_class:
    _target_: imfas.data.Dataset_Join_Dmajor
    masking_fn:
      _target_: imfas.utils.masking.mask_lcs_randomly
      _partial_: True

  test_dataset_class:
    _target_: imfas.data.Dataset_Join_Dmajor
    # TODO @TIM alex proposed to hold it fix for test, but do the imfas procedure

  train_dataloader_class:
    batch_size: 10
    shuffle: True

  test_dataloader_class:
    batch_size: 10
    shuffle: False

model:
  _target_: imfas.models.plackett_test.PlackettTest
  encoder:
    # linear model:
    _target_: imfas.utils.mlp.MLP
    hidden_dims:
      - ${dynamically_computed.n_data_meta_features}
      - 100
      - ${dynamically_computed.n_algos}
    activation: 'relu'


hydra:
  job:
    env_set:
      CUDA_VISIBLE_DEVICES: ""
trainer:
  trainerobj:
    _target_: imfas.trainer.base_trainer.BaseTrainer # todo switch out for Slice evaluator!

    optimizer:
      _target_: torch.optim.Adam
      lr: 0.001
      _partial_: True

  run_call:
    epochs: 10
    train_loss_fn:
      #      _target_: imfas.losses.plackett_luce.PlackettLuceLoss
      #      k: 10
      #      _target_: imfas.losses.spearman.SpearmanLoss

      # Documentation: https://github.com/allegro/allRank/blob/master/allrank/models/losses/neuralNDCG.py
      _target_: allrank.models.losses.neuralNDCG
      #    """
      #    NeuralNDCG loss introduced in "NeuralNDCG: Direct Optimisation of a Ranking Metric via Differentiable
      #    Relaxation of Sorting" - https://arxiv.org/abs/2102.07831. Based on the NeuralSort algorithm.
      #    :param y_pred: predictions from the model, shape [batch_size, slate_length]
      #    :param y_true: ground truth labels, shape [batch_size, slate_length]
      #    :param padded_value_indicator: an indicator of the y_true index containing a padded item, e.g. -1
      #    :param temperature: temperature for the NeuralSort algorithm
      #    :param powered_relevancies: whether to apply 2^x - 1 gain function, x otherwise
      #    :param k: rank at which the loss is truncated
      #    :param stochastic: whether to calculate the stochastic variant
      #    :param n_samples: how many stochastic samples are taken, used if stochastic == True
      #    :param beta: beta parameter for NeuralSort algorithm, used if stochastic == True
      #    :param log_scores: log_scores parameter for NeuralSort algorithm, used if stochastic == True
      #    :return: loss value, a torch.Tensor
      #    """
      _partial_: True
      padded_value_indicator: -1
      temperature: 1.
      powered_relevancies: True
      k: 10
      stochastic: False
      n_samples: 32
      beta: 0.1
      log_scores: True

    valid_loss_fns:
      top1_regret:
        _target_: imfas.evaluation.topk_regret.TopkMaxRegret
        k: 1
      top3_regret:
        _target_: imfas.evaluation.topk_regret.TopkMaxRegret
        k: 3
      plackett_luce3:
        _target_: imfas.losses.plackett_luce.PlackettLuceLoss
        k: 3
      plackett_luce10:
        _target_: imfas.losses.plackett_luce.PlackettLuceLoss
        k: 10
      #      spearman:
      #        _target_: imfas.losses.spearman.SpearmanLoss  # torchsort does not work with cpu!
      ndcg3:
        _target_: sklearn.metrics.ndcg_score
        _partial_: True
        k: 3
      ndcg1:
        _target_: sklearn.metrics.ndcg_score
        _partial_: True
        k: 1




