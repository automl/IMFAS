# @package _global_
# FIXME: currently only the current configuration dataset is cached
#  any alteration to this config require a dataset_raw.enable: true to then cache
#  a better way to do it would be to once get all the available _fidelities in one df
#  and then lazily load from there! This will reduce the burdon of recompute!
defaults:
  - override /dataset: lcbench
  - override /dataset/dataset_meta: lcbench_minimal
  - override /model: successive_halving
  - override /trainer: successive_halving

# actual experiment
model:
  estimator:
    slices:
      split: [ ] # datasets can be explicitly specified here, but should be computed based on
  #      train/test split function's seeding such that baseline uses same datasets!
  param_grid:
    algo_id: [ ] # must be computed during runtime due to ensembling happening in preprocessing
  resource: 'budget'
  max_resources: 51
  min_resources: 1  # cannot be zero due to sklearn's constraints on that parameter
  factor: 2


dataset:
  # when changing the slices, make sure to load them first!
  # loading all lcbench slices
  slices: [ 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,
            24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44,
            45, 46, 47, 48, 49, 50, 51 ]
  metric: Train/val_accuracy

  dataset_raw:
    enable: False

wandb:
  group: gravity_full
  notes: 'Running successive halving on lcbench raw'
  mode: 'online'
